{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c60e606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# UCI-HAR + ELK Backbone + Sequential Cross-Attention + Temporal Prototype Attention\n",
    "# Modified: ModernTCN → ELKBlock with Structural Reparameterization\n",
    "# ============================================================\n",
    "import os, math, time, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# ---------------------------\n",
    "# 0) UCI-HAR Loader (변경 없음)\n",
    "# ---------------------------\n",
    "_UCI_CHANNELS = [\n",
    "    (\"Inertial Signals/total_acc_x_\",  \"txt\"),\n",
    "    (\"Inertial Signals/total_acc_y_\",  \"txt\"),\n",
    "    (\"Inertial Signals/total_acc_z_\",  \"txt\"),\n",
    "    (\"Inertial Signals/body_acc_x_\",   \"txt\"),\n",
    "    (\"Inertial Signals/body_acc_y_\",   \"txt\"),\n",
    "    (\"Inertial Signals/body_acc_z_\",   \"txt\"),\n",
    "    (\"Inertial Signals/body_gyro_x_\",  \"txt\"),\n",
    "    (\"Inertial Signals/body_gyro_y_\",  \"txt\"),\n",
    "    (\"Inertial Signals/body_gyro_z_\",  \"txt\"),\n",
    "]\n",
    "\n",
    "def _read_txt_matrix(path: str) -> np.ndarray:\n",
    "    \".txt를 Numpy array로 읽는 헬퍼\"\n",
    "    return np.loadtxt(path)\n",
    "\n",
    "def _load_ucihar_split(root: str, split: str):\n",
    "    \"train, test 불러오는 함수\"\n",
    "    assert split in (\"train\", \"test\")\n",
    "    split_dir = os.path.join(root, split)\n",
    "    mats = []\n",
    "    for base, ext in _UCI_CHANNELS:\n",
    "        arr = _read_txt_matrix(os.path.join(split_dir, f\"{base}{split}.{ext}\"))\n",
    "        mats.append(arr[:, None, :])\n",
    "    X = np.concatenate(mats, axis=1).astype(np.float32)\n",
    "    y = np.loadtxt(os.path.join(split_dir, f\"y_{split}.txt\")).astype(np.int64) - 1\n",
    "    return X, y\n",
    "\n",
    "def fit_channel_stats(X: np.ndarray):\n",
    "    \"평균, 표준편차 계산 함수\"\n",
    "    mu = X.mean(axis=(0, 2), keepdims=True)\n",
    "    sd = X.std(axis=(0, 2), keepdims=True)\n",
    "    sd[sd < 1e-6] = 1.0\n",
    "    return mu.astype(np.float32), sd.astype(np.float32)\n",
    "\n",
    "class UCIHARDataset(Dataset):\n",
    "    def __init__(self, root: str, split: str, stats=None):\n",
    "        self.X, self.y = _load_ucihar_split(root, split)  # 데이터 로드\n",
    "        self.stats = stats  # 정규화 통계 (평균, 표준편차)\n",
    "\n",
    "    def set_stats(self, stats):\n",
    "        \"외부에서 계산된 통계(주로 훈련셋의 통계)를 설정하는 함수\"\n",
    "        self.stats = stats\n",
    "\n",
    "    def __len__(self): return len(self.X)  # 전체 샘플 수\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]  # idx번째 샘플 (9, 128)\n",
    "        y = self.y[idx]  # idx번째 라벨 (스칼라 값)\n",
    "\n",
    "        # 만약 정규화 통계가 설정되어 있다면\n",
    "        if self.stats is not None:\n",
    "            mu, sd = self.stats\n",
    "            # (9, 128) - (9, 1) / (9, 1) -> 브로드캐스팅으로 정규화 수행\n",
    "            x = (x - mu.squeeze(0)) / sd.squeeze(0)\n",
    "        return torch.from_numpy(x).float(), torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# ---------------------------\n",
    "# 1) ELK Backbone (신규 추가)\n",
    "# ---------------------------\n",
    "class ELKBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Efficient Large Kernel Block with structural reparameterization.\n",
    "    1. DataLoader에서 (B, 9, 128) tensor 'x'가 입력됨.\n",
    "    2. \n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=31, deploy=False):\n",
    "        super().__init__()\n",
    "        self.deploy = deploy  # deploy=True이면 '추론' 모드, False이면 '훈련' 모드\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size  # 큰 커널 크기 (예: 31)\n",
    "\n",
    "        # Calculate paddings (5개 브랜치에 필요한 커널 크기와 패딩 미리 계산)\n",
    "        padding_large1 = kernel_size // 2\n",
    "        kernel_size_large2 = kernel_size - 2\n",
    "        padding_large2 = kernel_size_large2 // 2\n",
    "        kernel_size_small1 = 5\n",
    "        padding_small1 = kernel_size_small1 // 2\n",
    "        kernel_size_small2 = 3\n",
    "        padding_small2 = kernel_size_small2 // 2\n",
    "\n",
    "        if deploy:\n",
    "            # 🚀 추론 모드\n",
    "            # 5개 브랜치가 합쳐진 '단 하나의' Conv 레이어만 정의\n",
    "            self.reparam_conv = nn.Conv1d(\n",
    "                in_channels, in_channels, kernel_size,\n",
    "                padding=padding_large1, groups=in_channels, bias=True\n",
    "            )\n",
    "        else:\n",
    "            # 🧠 훈련 모드(deploy=False):\n",
    "            # 5개의 병렬 브랜치를 모두 정의\n",
    "            self.dw_large1 = nn.Conv1d(\n",
    "                in_channels, in_channels, kernel_size,\n",
    "                padding=padding_large1, groups=in_channels, bias=False\n",
    "            )\n",
    "            self.bn_large1 = nn.BatchNorm1d(in_channels)\n",
    "\n",
    "            self.dw_large2 = nn.Conv1d(\n",
    "                in_channels, in_channels, kernel_size_large2,\n",
    "                padding=padding_large2, groups=in_channels, bias=False\n",
    "            )\n",
    "            self.bn_large2 = nn.BatchNorm1d(in_channels)\n",
    "\n",
    "            self.dw_small1 = nn.Conv1d(\n",
    "                in_channels, in_channels, kernel_size_small1,\n",
    "                padding=padding_small1, groups=in_channels, bias=False\n",
    "            )\n",
    "            self.bn_small1 = nn.BatchNorm1d(in_channels)\n",
    "\n",
    "            self.dw_small2 = nn.Conv1d(\n",
    "                in_channels, in_channels, kernel_size_small2,\n",
    "                padding=padding_small2, groups=in_channels, bias=False\n",
    "            )\n",
    "            self.bn_small2 = nn.BatchNorm1d(in_channels)\n",
    "\n",
    "            self.bn_id = nn.BatchNorm1d(in_channels)\n",
    "\n",
    "        # 5개 브랜치의 출력이 합쳐진 후, 공통으로 통과하는 1x1 Pointwise Conv\n",
    "        self.pointwise = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "        )\n",
    "        self.activation = nn.GELU() # ReLU 대신 GELU 사용\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.deploy:\n",
    "            x = self.reparam_conv(x)\n",
    "        else:\n",
    "            x1 = self.bn_large1(self.dw_large1(x))\n",
    "            x2 = self.bn_large2(self.dw_large2(x))\n",
    "            x3 = self.bn_small1(self.dw_small1(x))\n",
    "            x4 = self.bn_small2(self.dw_small2(x))\n",
    "            x5 = self.bn_id(x)\n",
    "            x = x1 + x2 + x3 + x4 + x5\n",
    "\n",
    "        x = self.activation(x)\n",
    "        return self.pointwise(x)\n",
    "\n",
    "    # 훈련 -> 추론 모드로 '변신'하는 함수\n",
    "    def reparameterize(self):\n",
    "        if self.deploy:  # 이미 추론 모드면 아무것도 안 함\n",
    "            return  \n",
    "\n",
    "        # Conv + BN을 수학적으로 합쳐(fuse)주는 헬퍼 함수\n",
    "        def _fuse(conv, bn):\n",
    "            if conv is None:\n",
    "                # Identity 브랜치 (bn_id) 처리\n",
    "                # k=31짜리 Conv지만, 중앙에만 '1'이 있는 껍데기 커널 생성\n",
    "                kernel = torch.zeros(\n",
    "                    (self.in_channels, 1, self.kernel_size),\n",
    "                    dtype=bn.weight.dtype, device=bn.weight.device\n",
    "                )\n",
    "                center = self.kernel_size // 2\n",
    "                kernel[:, 0, center] = 1.0\n",
    "                conv_bias = torch.zeros(\n",
    "                    self.in_channels, dtype=bn.weight.dtype, device=bn.weight.device\n",
    "                )\n",
    "            else:\n",
    "                # 일반 Conv 브랜치\n",
    "                kernel = conv.weight\n",
    "                conv_bias = torch.zeros(\n",
    "                    self.in_channels, dtype=bn.weight.dtype, device=bn.weight.device\n",
    "                )\n",
    "\n",
    "            # BN 파라미터(gamma, beta, mean, std)를 Conv의 가중치(weight)와 bias로 합침\n",
    "            std = (bn.running_var + bn.eps).sqrt()\n",
    "            gamma = bn.weight\n",
    "            beta = bn.bias\n",
    "            running_mean = bn.running_mean\n",
    "\n",
    "            fused_weight = kernel * (gamma / std).reshape(-1, 1, 1)\n",
    "            fused_bias = (gamma / std) * (conv_bias - running_mean) + beta\n",
    "\n",
    "            return fused_weight, fused_bias\n",
    "\n",
    "        # 5개 브랜치를 모두 Fusing\n",
    "        w_l1, b_l1 = _fuse(self.dw_large1, self.bn_large1)\n",
    "        w_l2, b_l2 = _fuse(self.dw_large2, self.bn_large2)\n",
    "        w_s1, b_s1 = _fuse(self.dw_small1, self.bn_small1)\n",
    "        w_s2, b_s2 = _fuse(self.dw_small2, self.bn_small2)\n",
    "        w_id, b_id = _fuse(None, self.bn_id)\n",
    "\n",
    "        # 커널 크기가 작은 브랜치들(l2, s1, s2)의 가중치에 패딩(0)을 추가   \n",
    "        # (k=31) + (k=29) -> (k=31) + (padding + k=29 + padding)\n",
    "        pad_l2 = (self.kernel_size - self.dw_large2.kernel_size[0]) // 2\n",
    "        w_l2 = F.pad(w_l2, (pad_l2, pad_l2))\n",
    "        pad_s1 = (self.kernel_size - self.dw_small1.kernel_size[0]) // 2\n",
    "        w_s1 = F.pad(w_s1, (pad_s1, pad_s1))\n",
    "        pad_s2 = (self.kernel_size - self.dw_small2.kernel_size[0]) // 2\n",
    "        w_s2 = F.pad(w_s2, (pad_s2, pad_s2))\n",
    "\n",
    "        # 5개 브랜치의 가중치와 bias를 모두 더함. 이게 최종 'reparam_conv'의 파라미터가 됨.\n",
    "        final_w = w_l1 + w_l2 + w_s1 + w_s2 + w_id\n",
    "        final_b = b_l1 + b_l2 + b_s1 + b_s2 + b_id\n",
    "\n",
    "        # 추론 모드용 reparam_conv를 생성\n",
    "        reparam_padding = self.kernel_size // 2\n",
    "        self.reparam_conv = nn.Conv1d(\n",
    "            self.in_channels, self.in_channels, self.kernel_size,\n",
    "            padding=reparam_padding, groups=self.in_channels, bias=True\n",
    "        ).to(final_w.device)\n",
    "\n",
    "        # 합쳐진 final_w와 final_b를 reparam_conv의 파라미터로 설정\n",
    "        self.reparam_conv.weight.data = final_w\n",
    "        self.reparam_conv.bias.data = final_b\n",
    "\n",
    "        # 🚀 '추론' 모드로 전환\n",
    "        self.deploy = True\n",
    "\n",
    "        # 훈련에만 쓰였던 5개 브랜치 모듈들을 메모리에서 삭제\n",
    "        for attr in ['dw_large1', 'bn_large1', 'dw_large2', 'bn_large2',\n",
    "                     'dw_small1', 'bn_small1', 'dw_small2', 'bn_small2', 'bn_id']:\n",
    "            if hasattr(self, attr):\n",
    "                delattr(self, attr)\n",
    "\n",
    "class ELKBackbone(nn.Module):\n",
    "    \"\"\"\n",
    "    ELK Backbone built by stacking ELKBlocks\n",
    "    `ELKBlock`을 여러 층 쌓아서 '백본'을 만듦\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=9, d_model=128, num_layers=6, kernel_size=31, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # 'stem': 입력 (9채널)을 모델의 기본 차원(d_model=128)으로 바꾸는 첫 번째 Conv\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, d_model, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm1d(d_model),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "        layers = []\n",
    "        # ELKBlock과 Dropout을 num_layers(6)번 반복해서 쌓음\n",
    "        for _ in range(num_layers):\n",
    "            layers.append(ELKBlock(d_model, d_model, kernel_size=kernel_size))\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        self.elk_layers = nn.Sequential(*layers)\n",
    "        self.out_channels = d_model  # 백본의 최종 출력 채널 수\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)  # (B, 9, 128) -> (B, 128, 128)\n",
    "        x = self.elk_layers(x)  # 6개 ELK 블록 통과 (B, 128, 128) -> (B, 128, 128)\n",
    "        return x\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Sequential Cross-Attention (변경 없음)\n",
    "# ---------------------------\n",
    "# LayerNorm의 경량화 버전인 RMSNorm\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.g = nn.Parameter(torch.ones(d))  # 학습 가능한 scale 파라미터\n",
    "    def forward(self, x):\n",
    "        # 입력 x를 제곱-평균-제곱근(RMS)으로 정규화하고 scale(g)을 곱함\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.g\n",
    "\n",
    "# 표준 멀티헤드 (크로스) 어텐션 모듈\n",
    "class MultiHeadCrossAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads=4, dropout=0.1, temperature=1.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.temperature = temperature\n",
    "\n",
    "        # Q(Query), K(Key), V(Value) 및 Output을 위한 Linear 프로젝션 레이어\n",
    "        self.q_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.k_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.v_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.out_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = (self.head_dim * temperature) ** -0.5  # 어텐션 스코어 스케일링 값\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        B, Tq, D = query.shape  # (Batch, Query길이, Dim)\n",
    "        _, Tkv, _ = key.shape  # (Batch, Key/Value길이, Dim)\n",
    "\n",
    "        # 1. Q, K, V를 프로젝션하고 (B, T, D) -> (B, Heads, T, HeadDim)로 변형\n",
    "        Q = self.q_proj(query).view(B, Tq, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.k_proj(key).view(B, Tkv, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.v_proj(value).view(B, Tkv, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # 2. 어텐션 스코어 계산 (Q @ K^T)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask.unsqueeze(1) == 0, float('-inf'))\n",
    "\n",
    "        # 3. Softmax\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # 4. 가중합 (Attention @ V)\n",
    "        out = torch.matmul(attn_weights, V)\n",
    "\n",
    "        # 5. (B, Heads, Tq, HeadDim) -> (B, Tq, D)로 원복 및 out_proj 통과\n",
    "        out = out.transpose(1, 2).contiguous().view(B, Tq, D)\n",
    "\n",
    "        return self.out_proj(out), attn_weights\n",
    "\n",
    "# 1차 어텐션: 센서/축(Axis) 전문가 토큰과 Cross-Attention\n",
    "class ImprovedSensorCrossAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_sensors=9, num_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_sensors = num_sensors\n",
    "\n",
    "        # '센서 전문가 토큰' (9개)과 '축 전문가 토큰' (3개)을 학습 가능한 파라미터로 생성\n",
    "        self.sensor_tokens = nn.Parameter(torch.randn(1, num_sensors, d_model) * 0.02)\n",
    "        self.axis_tokens = nn.Parameter(torch.randn(1, 3, d_model) * 0.02)\n",
    "\n",
    "        self.sensor_relation = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # 토큰들끼리 정보를 섞기 위한 Self-Attention (SA) 모듈\n",
    "        self.sensor_sa = nn.MultiheadAttention(d_model, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.axis_sa = nn.MultiheadAttention(d_model, num_heads, dropout=dropout, batch_first=True)\n",
    "\n",
    "        # '데이터(x)'가 '전문가 토큰'과 정보를 섞기 위한 Cross-Attention (CA) 모듈\n",
    "        self.cross_attn = MultiHeadCrossAttention(d_model, num_heads, dropout)\n",
    "\n",
    "\n",
    "        self.axis_projection = nn.Linear(d_model, d_model)\n",
    "        self.norm_s = RMSNorm(d_model)\n",
    "        self.norm_a = RMSNorm(d_model)\n",
    "        self.norm1 = RMSNorm(d_model)\n",
    "        self.norm2 = RMSNorm(d_model)\n",
    "\n",
    "        # 트랜스포머의 FFN (Feed-Forward Network)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model), nn.GELU(), nn.Dropout(dropout),\n",
    "            nn.Linear(4 * d_model, d_model), nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):  # x의 모양: (B, T=128, D=128)\n",
    "        B, T, D = x.shape\n",
    "        sensor_ctx = self.sensor_tokens.expand(B, -1, -1)\n",
    "        x_pooled = x.mean(dim=1, keepdim=True)\n",
    "        sensor_ctx = sensor_ctx + self.sensor_relation(x_pooled)\n",
    "        s_norm = self.norm_s(sensor_ctx)\n",
    "        s_attn, _ = self.sensor_sa(s_norm, s_norm, s_norm)\n",
    "        sensor_ctx = sensor_ctx + s_attn\n",
    "\n",
    "        x_axis = x.mean(dim=1)\n",
    "        axis_features = [x_axis for _ in range(3)]\n",
    "        axis_stack = torch.stack(axis_features, dim=1)\n",
    "\n",
    "        axis_ctx = self.axis_tokens.expand(B, -1, -1)\n",
    "        axis_ctx = axis_ctx + self.axis_projection(axis_stack)\n",
    "        a_norm = self.norm_a(axis_ctx)\n",
    "        a_attn, _ = self.axis_sa(a_norm, a_norm, a_norm)\n",
    "        axis_ctx = axis_ctx + a_attn\n",
    "\n",
    "        combined_ctx = torch.cat([sensor_ctx, axis_ctx], dim=1)\n",
    "        residual = x\n",
    "        x_norm = self.norm1(x)\n",
    "        attn_out, _ = self.cross_attn(x_norm, combined_ctx, combined_ctx)\n",
    "        x = residual + attn_out\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "# 2차 어텐션: 시간(Temporal) 축으로 Self-Attention\n",
    "class TemporalCrossAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads=4, dropout=0.1, causal=False):\n",
    "        super().__init__()\n",
    "        # CA 모듈을 Self-Attention (SA) 용도로 사용\n",
    "        self.cross_attn = MultiHeadCrossAttention(d_model, num_heads, dropout)\n",
    "        self.norm1 = RMSNorm(d_model)\n",
    "        self.norm2 = RMSNorm(d_model)\n",
    "        self.causal = causal\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model), nn.GELU(), nn.Dropout(dropout),\n",
    "            nn.Linear(4 * d_model, d_model), nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def _create_causal_mask(self, T, device):\n",
    "        return torch.tril(torch.ones(T, T, device=device))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, D = x.shape\n",
    "        mask = self._create_causal_mask(T, x.device) if self.causal else None\n",
    "        residual = x\n",
    "        x_norm = self.norm1(x)\n",
    "\n",
    "        # Query=x, Key=x, Value=x. 즉, Self-Attention\n",
    "        # \"128개 타임스텝들아, 너희끼리(x, x, x) 정보를 교환해\"\n",
    "        attn_out, _ = self.cross_attn(x_norm, x_norm, x_norm, mask)\n",
    "        x = residual + attn_out  # Residual\n",
    "        x = x + self.ffn(self.norm2(x))  # Residual + FFN\n",
    "        return x\n",
    "\n",
    "# 1차, 2차 어텐션을 순차적으로 실행\n",
    "class SequentialCrossAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_sensors=9, num_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.sensor_attn = ImprovedSensorCrossAttention(d_model, num_sensors, num_heads, dropout)\n",
    "        self.temporal_attn = TemporalCrossAttention(d_model, num_heads, dropout, causal=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sensor_attn(x)\n",
    "        x = self.temporal_attn(x)\n",
    "        return x\n",
    "\n",
    "# ---------------------------\n",
    "# 3) Temporal Prototype Attention (변경 없음) 어텐션으로 정제된 특징 맵을 '프로토타입'과 비교하여 압축합니다.\n",
    "# ---------------------------\n",
    "# TPA의 기본 로직\n",
    "class TemporalPrototypeAttention(nn.Module):\n",
    "    def __init__(self, dim, num_prototypes=16, seg_kernel=9, heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert dim % heads == 0\n",
    "        self.dim, self.heads, self.head_dim = dim, heads, dim // heads\n",
    "        self.num_prototypes = num_prototypes\n",
    "\n",
    "        # '프로토타입' (학습 가능한 행동 템플릿)을 파라미터로 생성\n",
    "        self.proto = nn.Parameter(torch.randn(num_prototypes, dim) * 0.02)\n",
    "        pad = (seg_kernel - 1) // 2\n",
    "        self.dw = nn.Conv1d(dim, dim, kernel_size=seg_kernel, padding=pad, groups=dim, bias=False)\n",
    "        self.pw = nn.Conv1d(dim, dim, kernel_size=1, bias=False)\n",
    "        # Q, K, V 프로젝션 레이어\n",
    "        self.q_proj, self.k_proj, self.v_proj, self.out_proj = [nn.Linear(dim, dim, bias=False) for _ in range(4)]\n",
    "        self.fuse = nn.Sequential(nn.Linear(dim, dim), nn.SiLU(), nn.Dropout(dropout), nn.Linear(dim, dim))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "    def _tpa_core(self, x, proto):\n",
    "        B, T, D = x.shape\n",
    "        P = proto.size(1) if proto.dim() == 3 else proto.size(0)\n",
    "        xloc = self.pw(self.dw(x.transpose(1, 2))).transpose(1, 2)\n",
    "        K, V = self.k_proj(xloc), self.v_proj(xloc)\n",
    "        Qp = self.q_proj(proto) if proto.dim() == 3 else self.q_proj(proto).unsqueeze(0).expand(B, -1, -1)\n",
    "\n",
    "        def split_heads(t, is_kv=False):\n",
    "            shape = (B, T if is_kv else P, self.heads, D // self.heads)\n",
    "            return t.view(*shape).transpose(1, 2)\n",
    "\n",
    "        Qh, Kh, Vh = split_heads(Qp, False), split_heads(K, True), split_heads(V, True)\n",
    "        scores = torch.matmul(Qh, Kh.transpose(-2, -1)) * self.scale\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        proto_tokens = torch.matmul(attn, Vh).transpose(1, 2).contiguous().view(B, P, D)\n",
    "        z = self.fuse(proto_tokens.mean(dim=1) + proto_tokens.max(dim=1).values)\n",
    "        z = self.out_proj(z)\n",
    "        aux = {\"attn\": attn, \"align_peak_mean\": attn.amax(dim=-1).mean().detach()}\n",
    "        return z, aux  # 최종 압축 벡터 z 반환\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._tpa_core(x, self.proto)\n",
    "\n",
    "\n",
    "# TPA의 '클래스 조건부' 업그레이드 버전\n",
    "class ClassConditionalTPA(TemporalPrototypeAttention):\n",
    "    def __init__(self, dim, num_classes, p_shared=8, p_class=4, **kw):\n",
    "        # '공유' 프로토타입 8개 + '클래스별' 프로토타입 (4 * 6=24개) = 총 32개\n",
    "        super().__init__(dim, num_prototypes=p_shared + num_classes * p_class, **kw)\n",
    "        self.p_shared, self.p_class, self.num_classes = p_shared, p_class, num_classes\n",
    "\n",
    "    # 특정 클래스(cls_idx)의 전용 프로토타입 4개를 가져오는 함수\n",
    "    def _slice_class(self, cls_idx):\n",
    "        base = self.p_shared + cls_idx * self.p_class\n",
    "        return self.proto[base: base + self.p_class]\n",
    "\n",
    "    def forward(self, x, y=None, logits=None):\n",
    "        B = x.size(0)\n",
    "        shared = self.proto[:self.p_shared]\n",
    "        if self.training and y is not None:\n",
    "            # 🧠 훈련 시: 정답(y)을 아니까, 정답 클래스의 전용 프로토타입을 가져옴\n",
    "            cls_proto = torch.stack([self._slice_class(y[i].item()) for i in range(B)], 0)\n",
    "            # (공통 8개 + 정답 전용 4개) = 12개 프로토타입을 Query로 사용\n",
    "            proto = torch.cat([shared.unsqueeze(0).expand(B, -1, -1), cls_proto], dim=1)\n",
    "        else:\n",
    "            # 🚀 평가 시: 정답(y)을 모름\n",
    "            if logits is None:\n",
    "                # (예외) logits도 없으면 공통 프로토타입 8개만 사용\n",
    "                proto = shared.unsqueeze(0).expand(B, -1, -1)\n",
    "            else:\n",
    "                # 백본이 예측한 logits (B, 6)를 softmax(pi) (B, 6)로 변환\n",
    "                pi = logits.softmax(dim=-1)\n",
    "                # 6개 클래스의 전용 프로토타입 뱅크 (6, 4, D)\n",
    "                class_bank = torch.stack([self._slice_class(c) for c in range(self.num_classes)], 0)\n",
    "                # (B, 6) @ (6, 4, D) -> (B, 4, D)\n",
    "                # 6개 클래스의 전용 프로토타입 4개를 logits 확률로 '섞어서' 만듦\n",
    "                mixed = torch.einsum('bc,cpd->bpd', pi, class_bank)\n",
    "                # (공통 8개 + 섞인 4개) = 12개 프로토타입을 Query로 사용\n",
    "                proto = torch.cat([shared.unsqueeze(0).expand(B, -1, -1), mixed], dim=1)\n",
    "\n",
    "        return self._tpa_core(x, proto)  # 선택된 'proto'를 Query로 사용하여 TPA 핵심 로직 수행\n",
    "\n",
    "# ---------------------------\n",
    "# 4) Loss Function (변경 없음)\n",
    "# ---------------------------\n",
    "class ImprovedClsLoss(nn.Module):\n",
    "    def __init__(self, use_focal=True, alpha=0.25, gamma=2.0, init_loss_weight=0.4, label_smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.use_focal, self.alpha, self.gamma = use_focal, alpha, gamma\n",
    "        self.init_loss_weight, self.label_smoothing = init_loss_weight, label_smoothing\n",
    "\n",
    "    def forward(self, logits, labels, aux_info, aux_weight_multiplier=1.0):\n",
    "        n_classes = logits.size(-1)\n",
    "        if self.label_smoothing > 0:\n",
    "            one_hot = F.one_hot(labels, num_classes=n_classes).float()\n",
    "            smooth_label = one_hot * (1 - self.label_smoothing) + self.label_smoothing / n_classes\n",
    "            if self.use_focal:\n",
    "                log_probs = F.log_softmax(logits, dim=-1)\n",
    "                pt = torch.exp(log_probs)\n",
    "                focal_weight = (1 - pt) ** self.gamma\n",
    "                loss = -(self.alpha * focal_weight * smooth_label * log_probs).sum(dim=-1).mean()\n",
    "            else:\n",
    "                loss = -(smooth_label * F.log_softmax(logits, dim=-1)).sum(dim=-1).mean()\n",
    "        else:\n",
    "            ce = F.cross_entropy(logits, labels, reduction=\"none\")\n",
    "            if self.use_focal:\n",
    "                pt = torch.exp(-ce)\n",
    "                loss = (self.alpha * (1 - pt)**self.gamma * ce).mean()\n",
    "            else:\n",
    "                loss = ce.mean()\n",
    "\n",
    "        total_loss = loss\n",
    "        if \"logits_init\" in aux_info:\n",
    "            loss_init = F.cross_entropy(aux_info[\"logits_init\"], labels)\n",
    "            total_loss = loss + self.init_loss_weight * loss_init\n",
    "\n",
    "        return total_loss, {\n",
    "            \"classification_loss\": float(loss.item()),\n",
    "            \"total_loss\": float(total_loss.item()),\n",
    "            \"align_peak_mean\": float(aux_info.get(\"align_peak_mean\", 0.)),\n",
    "        }\n",
    "\n",
    "# ---------------------------\n",
    "# 5) Hybrid Model (TCN -> ELK) 모든 부품을 조립하는 최종 모델\n",
    "# ---------------------------\n",
    "class ELK_SequentialAttn_TPA(nn.Module):\n",
    "    def __init__(self, nvars, seq_len, num_classes,\n",
    "                 num_elk_layers, elk_kernel_size, # ELK 파라미터\n",
    "                 d_model, heads, dropout,\n",
    "                 num_prototypes, seg_kernel, p_shared, p_class,\n",
    "                 use_class_conditional, use_cross_attention):\n",
    "        super().__init__()\n",
    "        self.use_class_conditional = use_class_conditional\n",
    "        self.use_cross_attention = use_cross_attention\n",
    "        \n",
    "        # 부품 1: ELKBackbone (섹션 1)\n",
    "        # TCN Backbone -> ELK Backbone 으로 교체\n",
    "        self.backbone = ELKBackbone(\n",
    "            in_channels=nvars,\n",
    "            d_model=d_model,\n",
    "            num_layers=num_elk_layers,\n",
    "            kernel_size=elk_kernel_size,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        # 백본 출력 채널(128) -> 어텐션 입력 채널(128)로 맞춰주는 1x1 Conv\n",
    "        self.proj = nn.Conv1d(self.backbone.out_channels, d_model, kernel_size=1)\n",
    "\n",
    "        # 부품 2: SequentialCrossAttention (섹션 2)\n",
    "        if use_cross_attention:\n",
    "            self.cross_attn = SequentialCrossAttention(\n",
    "                d_model, nvars, num_heads=heads, dropout=dropout\n",
    "            )\n",
    "\n",
    "        # 부품 3: TemporalPrototypeAttention (섹션 3)\n",
    "        if use_class_conditional:\n",
    "            self.tpa = ClassConditionalTPA(\n",
    "                d_model, num_classes, p_shared, p_class,\n",
    "                seg_kernel=seg_kernel, heads=heads, dropout=dropout  # 조건부 TPA\n",
    "            )\n",
    "        else:\n",
    "            self.tpa = TemporalPrototypeAttention(\n",
    "                d_model, num_prototypes, seg_kernel, heads, dropout  # 일반 TPA\n",
    "            )\n",
    "\n",
    "        self.head_init = nn.Linear(d_model, num_classes)  # 분류 헤드 1: 백본 출력을 위한 초기 분류기\n",
    "        self.head_final = nn.Linear(d_model, num_classes)  # 분류 헤드 2: TPA 출력을 위한 최종 분류기\n",
    "\n",
    "    def forward(self, x, labels=None):\n",
    "        x = self.backbone(x) # self.tcn -> self.backbone\n",
    "        x = self.proj(x)\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        x_pooled = x.mean(dim=1)\n",
    "        logits_init = self.head_init(x_pooled)\n",
    "\n",
    "        if self.use_cross_attention:\n",
    "            x = self.cross_attn(x)\n",
    "\n",
    "        if self.use_class_conditional:\n",
    "            z, aux = self.tpa(x, y=labels, logits=logits_init)\n",
    "        else:\n",
    "            z, aux = self.tpa(x)\n",
    "\n",
    "        logits_final = self.head_final(z)\n",
    "        aux['logits_init'] = logits_init\n",
    "        return logits_final, aux\n",
    "\n",
    "    def reparameterize(self):\n",
    "        \"\"\"추론을 위해 모든 ELK 블록을 재매개변수화합니다.\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, ELKBlock):\n",
    "                m.reparameterize()\n",
    "\n",
    "# ---------------------------\n",
    "# 6) Train / Eval (변경 없음)\n",
    "# ---------------------------\n",
    "def train_epoch(model, loader, criterion, optim, scheduler, device, accumulation_steps=4):\n",
    "    model.train()\n",
    "    tot, correct, total = 0.0, 0, 0\n",
    "    optim.zero_grad()\n",
    "    for i, (x, y) in enumerate(loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits, aux = model(x, labels=y)\n",
    "        loss, _ = criterion(logits, y, aux, 1.0)\n",
    "        loss = loss / accumulation_steps\n",
    "        loss.backward()\n",
    "\n",
    "        if (i + 1) % accumulation_steps == 0 or i == len(loader) - 1:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optim.step()\n",
    "            if scheduler: scheduler.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "        tot += loss.item() * accumulation_steps\n",
    "        pred = logits.argmax(1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    return tot / len(loader), 100 * correct / total\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    tot, correct, total = 0.0, 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits, aux = model(x, labels=None)\n",
    "        loss, _ = criterion(logits, y, aux, 1.0)\n",
    "        tot += loss.item()\n",
    "        pred = logits.argmax(1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    return tot / len(loader), 100 * correct / total\n",
    "\n",
    "# ---------------------------\n",
    "# 7) Main (모델 및 파라미터 수정)\n",
    "# ---------------------------\n",
    "def main():\n",
    "    import gc\n",
    "    torch.cuda.empty_cache(); gc.collect()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(\"UCI-HAR Training: ELK Backbone + Sequential Cross-Attention + TPA\")\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "    uci_root = \"/content/drive/MyDrive/Colab Notebooks/UCI-HAR/UCI-HAR\"\n",
    "\n",
    "    if not os.path.exists(uci_root):\n",
    "        print(f\"Error: UCI-HAR data not found at '{uci_root}'\")\n",
    "        return\n",
    "\n",
    "    train_ds = UCIHARDataset(uci_root, \"train\")\n",
    "    test_ds = UCIHARDataset(uci_root, \"test\")\n",
    "\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        np.arange(len(train_ds)), test_size=0.2, random_state=42, stratify=train_ds.y\n",
    "    )\n",
    "\n",
    "    mu, sd = fit_channel_stats(train_ds.X[train_indices])\n",
    "    train_ds.set_stats((mu, sd))\n",
    "    test_ds.set_stats((mu, sd)) # 테스트셋에도 훈련셋 통계 적용\n",
    "\n",
    "    # Validation셋을 위한 별도 Dataset 객체 생성 후 통계 적용\n",
    "    val_ds = UCIHARDataset(uci_root, \"train\")\n",
    "    val_ds.set_stats((mu, sd))\n",
    "\n",
    "    train_subset = Subset(train_ds, train_indices)\n",
    "    val_subset = Subset(val_ds, val_indices)\n",
    "\n",
    "    print(f\"\\nDataset Split: Train={len(train_subset)}, Val={len(val_subset)}, Test={len(test_ds)}\")\n",
    "\n",
    "    # 모델 정의 (ELK 파라미터 사용)\n",
    "    model = ELK_SequentialAttn_TPA(\n",
    "        nvars=9,\n",
    "        seq_len=128,\n",
    "        num_classes=6,\n",
    "        d_model=128,\n",
    "        heads=4,\n",
    "        dropout=0.2,\n",
    "        # --- ELK Parameters ---\n",
    "        num_elk_layers=6,\n",
    "        elk_kernel_size=31,\n",
    "        # --- TPA Parameters ---\n",
    "        num_prototypes=8,\n",
    "        seg_kernel=3,\n",
    "        p_shared=6,\n",
    "        p_class=4,\n",
    "        use_class_conditional=True,\n",
    "        use_cross_attention=True\n",
    "    ).to(device)\n",
    "\n",
    "    print(f\"\\nModel parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(\"Architecture: ELK Backbone + Sequential Cross-Attention + TPA\")\n",
    "\n",
    "    criterion = ImprovedClsLoss(use_focal=True, alpha=0.25, gamma=2.0, init_loss_weight=0.4, label_smoothing=0.1)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.05)\n",
    "\n",
    "    batch_size = 64\n",
    "    accumulation_steps = 2\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=batch_size*2, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size*2, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "    max_epochs = 60\n",
    "    total_steps = (len(train_loader) // accumulation_steps) * max_epochs\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, max_lr=2e-3, total_steps=total_steps, pct_start=0.2\n",
    "    )\n",
    "\n",
    "    best_val_acc, best_epoch = 0.0, 0\n",
    "    print(f\"\\nStarting training for {max_epochs} epochs...\")\n",
    "    print(f\"Effective batch size: {batch_size * accumulation_steps}\")\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"\\nEpoch {epoch}/{max_epochs} | LR: {lr:.6f}\")\n",
    "        tr_loss, tr_acc = train_epoch(model, train_loader, criterion, optimizer, scheduler, device, accumulation_steps)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "        print(f\"  Train: loss {tr_loss:.4f} | acc {tr_acc:.2f}%\")\n",
    "        print(f\"  Val  : loss {val_loss:.4f} | acc {val_acc:.2f}%\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc, best_epoch = val_acc, epoch\n",
    "            torch.save(model.state_dict(), \"best_model_elk_valsel.pth\")\n",
    "            print(f\"  ✓ New best validation accuracy: {best_val_acc:.2f}% (model saved)\")\n",
    "\n",
    "    print(f\"\\nTraining completed! Best val acc: {best_val_acc:.2f}% at epoch {best_epoch}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL TEST SET EVALUATION (Unseen Data)\")\n",
    "    print(\"→ Reparameterizing ELK blocks for inference speed-up...\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # 저장된 최고의 모델을 불러온 후, 재매개변수화 수행\n",
    "    model.load_state_dict(torch.load(\"best_model_elk_valsel.pth\"))\n",
    "    model.reparameterize()\n",
    "\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "    print(f\"\\nFinal Results on Test Set:\")\n",
    "    print(f\"  - Test Loss    : {test_loss:.4f}\")\n",
    "    print(f\"  - Test Accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
