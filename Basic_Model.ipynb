{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c60e606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# UCI-HAR + ELK Backbone + Sequential Cross-Attention + Temporal Prototype Attention\n",
    "# Modified: ModernTCN â†’ ELKBlock with Structural Reparameterization\n",
    "# ============================================================\n",
    "import os, math, time, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# ---------------------------\n",
    "# 0) UCI-HAR Loader (ë³€ê²½ ì—†ìŒ)\n",
    "# ---------------------------\n",
    "_UCI_CHANNELS = [\n",
    "    (\"Inertial Signals/total_acc_x_\",  \"txt\"),\n",
    "    (\"Inertial Signals/total_acc_y_\",  \"txt\"),\n",
    "    (\"Inertial Signals/total_acc_z_\",  \"txt\"),\n",
    "    (\"Inertial Signals/body_acc_x_\",   \"txt\"),\n",
    "    (\"Inertial Signals/body_acc_y_\",   \"txt\"),\n",
    "    (\"Inertial Signals/body_acc_z_\",   \"txt\"),\n",
    "    (\"Inertial Signals/body_gyro_x_\",  \"txt\"),\n",
    "    (\"Inertial Signals/body_gyro_y_\",  \"txt\"),\n",
    "    (\"Inertial Signals/body_gyro_z_\",  \"txt\"),\n",
    "]\n",
    "\n",
    "def _read_txt_matrix(path: str) -> np.ndarray:\n",
    "    \".txtë¥¼ Numpy arrayë¡œ ì½ëŠ” í—¬í¼\"\n",
    "    return np.loadtxt(path)\n",
    "\n",
    "def _load_ucihar_split(root: str, split: str):\n",
    "    \"train, test ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜\"\n",
    "    assert split in (\"train\", \"test\")\n",
    "    split_dir = os.path.join(root, split)\n",
    "    mats = []\n",
    "    for base, ext in _UCI_CHANNELS:\n",
    "        arr = _read_txt_matrix(os.path.join(split_dir, f\"{base}{split}.{ext}\"))\n",
    "        mats.append(arr[:, None, :])\n",
    "    X = np.concatenate(mats, axis=1).astype(np.float32)\n",
    "    y = np.loadtxt(os.path.join(split_dir, f\"y_{split}.txt\")).astype(np.int64) - 1\n",
    "    return X, y\n",
    "\n",
    "def fit_channel_stats(X: np.ndarray):\n",
    "    \"í‰ê· , í‘œì¤€í¸ì°¨ ê³„ì‚° í•¨ìˆ˜\"\n",
    "    mu = X.mean(axis=(0, 2), keepdims=True)\n",
    "    sd = X.std(axis=(0, 2), keepdims=True)\n",
    "    sd[sd < 1e-6] = 1.0\n",
    "    return mu.astype(np.float32), sd.astype(np.float32)\n",
    "\n",
    "class UCIHARDataset(Dataset):\n",
    "    def __init__(self, root: str, split: str, stats=None):\n",
    "        self.X, self.y = _load_ucihar_split(root, split)  # ë°ì´í„° ë¡œë“œ\n",
    "        self.stats = stats  # ì •ê·œí™” í†µê³„ (í‰ê· , í‘œì¤€í¸ì°¨)\n",
    "\n",
    "    def set_stats(self, stats):\n",
    "        \"ì™¸ë¶€ì—ì„œ ê³„ì‚°ëœ í†µê³„(ì£¼ë¡œ í›ˆë ¨ì…‹ì˜ í†µê³„)ë¥¼ ì„¤ì •í•˜ëŠ” í•¨ìˆ˜\"\n",
    "        self.stats = stats\n",
    "\n",
    "    def __len__(self): return len(self.X)  # ì „ì²´ ìƒ˜í”Œ ìˆ˜\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]  # idxë²ˆì§¸ ìƒ˜í”Œ (9, 128)\n",
    "        y = self.y[idx]  # idxë²ˆì§¸ ë¼ë²¨ (ìŠ¤ì¹¼ë¼ ê°’)\n",
    "\n",
    "        # ë§Œì•½ ì •ê·œí™” í†µê³„ê°€ ì„¤ì •ë˜ì–´ ìˆë‹¤ë©´\n",
    "        if self.stats is not None:\n",
    "            mu, sd = self.stats\n",
    "            # (9, 128) - (9, 1) / (9, 1) -> ë¸Œë¡œë“œìºìŠ¤íŒ…ìœ¼ë¡œ ì •ê·œí™” ìˆ˜í–‰\n",
    "            x = (x - mu.squeeze(0)) / sd.squeeze(0)\n",
    "        return torch.from_numpy(x).float(), torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# ---------------------------\n",
    "# 1) ELK Backbone (ì‹ ê·œ ì¶”ê°€)\n",
    "# ---------------------------\n",
    "class ELKBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Efficient Large Kernel Block with structural reparameterization.\n",
    "    1. DataLoaderì—ì„œ (B, 9, 128) tensor 'x'ê°€ ì…ë ¥ë¨.\n",
    "    2. \n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=31, deploy=False):\n",
    "        super().__init__()\n",
    "        self.deploy = deploy  # deploy=Trueì´ë©´ 'ì¶”ë¡ ' ëª¨ë“œ, Falseì´ë©´ 'í›ˆë ¨' ëª¨ë“œ\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size  # í° ì»¤ë„ í¬ê¸° (ì˜ˆ: 31)\n",
    "\n",
    "        # Calculate paddings (5ê°œ ë¸Œëœì¹˜ì— í•„ìš”í•œ ì»¤ë„ í¬ê¸°ì™€ íŒ¨ë”© ë¯¸ë¦¬ ê³„ì‚°)\n",
    "        padding_large1 = kernel_size // 2\n",
    "        kernel_size_large2 = kernel_size - 2\n",
    "        padding_large2 = kernel_size_large2 // 2\n",
    "        kernel_size_small1 = 5\n",
    "        padding_small1 = kernel_size_small1 // 2\n",
    "        kernel_size_small2 = 3\n",
    "        padding_small2 = kernel_size_small2 // 2\n",
    "\n",
    "        if deploy:\n",
    "            # ğŸš€ ì¶”ë¡  ëª¨ë“œ\n",
    "            # 5ê°œ ë¸Œëœì¹˜ê°€ í•©ì³ì§„ 'ë‹¨ í•˜ë‚˜ì˜' Conv ë ˆì´ì–´ë§Œ ì •ì˜\n",
    "            self.reparam_conv = nn.Conv1d(\n",
    "                in_channels, in_channels, kernel_size,\n",
    "                padding=padding_large1, groups=in_channels, bias=True\n",
    "            )\n",
    "        else:\n",
    "            # ğŸ§  í›ˆë ¨ ëª¨ë“œ(deploy=False):\n",
    "            # 5ê°œì˜ ë³‘ë ¬ ë¸Œëœì¹˜ë¥¼ ëª¨ë‘ ì •ì˜\n",
    "            self.dw_large1 = nn.Conv1d(\n",
    "                in_channels, in_channels, kernel_size,\n",
    "                padding=padding_large1, groups=in_channels, bias=False\n",
    "            )\n",
    "            self.bn_large1 = nn.BatchNorm1d(in_channels)\n",
    "\n",
    "            self.dw_large2 = nn.Conv1d(\n",
    "                in_channels, in_channels, kernel_size_large2,\n",
    "                padding=padding_large2, groups=in_channels, bias=False\n",
    "            )\n",
    "            self.bn_large2 = nn.BatchNorm1d(in_channels)\n",
    "\n",
    "            self.dw_small1 = nn.Conv1d(\n",
    "                in_channels, in_channels, kernel_size_small1,\n",
    "                padding=padding_small1, groups=in_channels, bias=False\n",
    "            )\n",
    "            self.bn_small1 = nn.BatchNorm1d(in_channels)\n",
    "\n",
    "            self.dw_small2 = nn.Conv1d(\n",
    "                in_channels, in_channels, kernel_size_small2,\n",
    "                padding=padding_small2, groups=in_channels, bias=False\n",
    "            )\n",
    "            self.bn_small2 = nn.BatchNorm1d(in_channels)\n",
    "\n",
    "            self.bn_id = nn.BatchNorm1d(in_channels)\n",
    "\n",
    "        # 5ê°œ ë¸Œëœì¹˜ì˜ ì¶œë ¥ì´ í•©ì³ì§„ í›„, ê³µí†µìœ¼ë¡œ í†µê³¼í•˜ëŠ” 1x1 Pointwise Conv\n",
    "        self.pointwise = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "        )\n",
    "        self.activation = nn.GELU() # ReLU ëŒ€ì‹  GELU ì‚¬ìš©\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.deploy:\n",
    "            x = self.reparam_conv(x)\n",
    "        else:\n",
    "            x1 = self.bn_large1(self.dw_large1(x))\n",
    "            x2 = self.bn_large2(self.dw_large2(x))\n",
    "            x3 = self.bn_small1(self.dw_small1(x))\n",
    "            x4 = self.bn_small2(self.dw_small2(x))\n",
    "            x5 = self.bn_id(x)\n",
    "            x = x1 + x2 + x3 + x4 + x5\n",
    "\n",
    "        x = self.activation(x)\n",
    "        return self.pointwise(x)\n",
    "\n",
    "    # í›ˆë ¨ -> ì¶”ë¡  ëª¨ë“œë¡œ 'ë³€ì‹ 'í•˜ëŠ” í•¨ìˆ˜\n",
    "    def reparameterize(self):\n",
    "        if self.deploy:  # ì´ë¯¸ ì¶”ë¡  ëª¨ë“œë©´ ì•„ë¬´ê²ƒë„ ì•ˆ í•¨\n",
    "            return  \n",
    "\n",
    "        # Conv + BNì„ ìˆ˜í•™ì ìœ¼ë¡œ í•©ì³(fuse)ì£¼ëŠ” í—¬í¼ í•¨ìˆ˜\n",
    "        def _fuse(conv, bn):\n",
    "            if conv is None:\n",
    "                # Identity ë¸Œëœì¹˜ (bn_id) ì²˜ë¦¬\n",
    "                # k=31ì§œë¦¬ Convì§€ë§Œ, ì¤‘ì•™ì—ë§Œ '1'ì´ ìˆëŠ” ê»ë°ê¸° ì»¤ë„ ìƒì„±\n",
    "                kernel = torch.zeros(\n",
    "                    (self.in_channels, 1, self.kernel_size),\n",
    "                    dtype=bn.weight.dtype, device=bn.weight.device\n",
    "                )\n",
    "                center = self.kernel_size // 2\n",
    "                kernel[:, 0, center] = 1.0\n",
    "                conv_bias = torch.zeros(\n",
    "                    self.in_channels, dtype=bn.weight.dtype, device=bn.weight.device\n",
    "                )\n",
    "            else:\n",
    "                # ì¼ë°˜ Conv ë¸Œëœì¹˜\n",
    "                kernel = conv.weight\n",
    "                conv_bias = torch.zeros(\n",
    "                    self.in_channels, dtype=bn.weight.dtype, device=bn.weight.device\n",
    "                )\n",
    "\n",
    "            # BN íŒŒë¼ë¯¸í„°(gamma, beta, mean, std)ë¥¼ Convì˜ ê°€ì¤‘ì¹˜(weight)ì™€ biasë¡œ í•©ì¹¨\n",
    "            std = (bn.running_var + bn.eps).sqrt()\n",
    "            gamma = bn.weight\n",
    "            beta = bn.bias\n",
    "            running_mean = bn.running_mean\n",
    "\n",
    "            fused_weight = kernel * (gamma / std).reshape(-1, 1, 1)\n",
    "            fused_bias = (gamma / std) * (conv_bias - running_mean) + beta\n",
    "\n",
    "            return fused_weight, fused_bias\n",
    "\n",
    "        # 5ê°œ ë¸Œëœì¹˜ë¥¼ ëª¨ë‘ Fusing\n",
    "        w_l1, b_l1 = _fuse(self.dw_large1, self.bn_large1)\n",
    "        w_l2, b_l2 = _fuse(self.dw_large2, self.bn_large2)\n",
    "        w_s1, b_s1 = _fuse(self.dw_small1, self.bn_small1)\n",
    "        w_s2, b_s2 = _fuse(self.dw_small2, self.bn_small2)\n",
    "        w_id, b_id = _fuse(None, self.bn_id)\n",
    "\n",
    "        # ì»¤ë„ í¬ê¸°ê°€ ì‘ì€ ë¸Œëœì¹˜ë“¤(l2, s1, s2)ì˜ ê°€ì¤‘ì¹˜ì— íŒ¨ë”©(0)ì„ ì¶”ê°€   \n",
    "        # (k=31) + (k=29) -> (k=31) + (padding + k=29 + padding)\n",
    "        pad_l2 = (self.kernel_size - self.dw_large2.kernel_size[0]) // 2\n",
    "        w_l2 = F.pad(w_l2, (pad_l2, pad_l2))\n",
    "        pad_s1 = (self.kernel_size - self.dw_small1.kernel_size[0]) // 2\n",
    "        w_s1 = F.pad(w_s1, (pad_s1, pad_s1))\n",
    "        pad_s2 = (self.kernel_size - self.dw_small2.kernel_size[0]) // 2\n",
    "        w_s2 = F.pad(w_s2, (pad_s2, pad_s2))\n",
    "\n",
    "        # 5ê°œ ë¸Œëœì¹˜ì˜ ê°€ì¤‘ì¹˜ì™€ biasë¥¼ ëª¨ë‘ ë”í•¨. ì´ê²Œ ìµœì¢… 'reparam_conv'ì˜ íŒŒë¼ë¯¸í„°ê°€ ë¨.\n",
    "        final_w = w_l1 + w_l2 + w_s1 + w_s2 + w_id\n",
    "        final_b = b_l1 + b_l2 + b_s1 + b_s2 + b_id\n",
    "\n",
    "        # ì¶”ë¡  ëª¨ë“œìš© reparam_convë¥¼ ìƒì„±\n",
    "        reparam_padding = self.kernel_size // 2\n",
    "        self.reparam_conv = nn.Conv1d(\n",
    "            self.in_channels, self.in_channels, self.kernel_size,\n",
    "            padding=reparam_padding, groups=self.in_channels, bias=True\n",
    "        ).to(final_w.device)\n",
    "\n",
    "        # í•©ì³ì§„ final_wì™€ final_bë¥¼ reparam_convì˜ íŒŒë¼ë¯¸í„°ë¡œ ì„¤ì •\n",
    "        self.reparam_conv.weight.data = final_w\n",
    "        self.reparam_conv.bias.data = final_b\n",
    "\n",
    "        # ğŸš€ 'ì¶”ë¡ ' ëª¨ë“œë¡œ ì „í™˜\n",
    "        self.deploy = True\n",
    "\n",
    "        # í›ˆë ¨ì—ë§Œ ì“°ì˜€ë˜ 5ê°œ ë¸Œëœì¹˜ ëª¨ë“ˆë“¤ì„ ë©”ëª¨ë¦¬ì—ì„œ ì‚­ì œ\n",
    "        for attr in ['dw_large1', 'bn_large1', 'dw_large2', 'bn_large2',\n",
    "                     'dw_small1', 'bn_small1', 'dw_small2', 'bn_small2', 'bn_id']:\n",
    "            if hasattr(self, attr):\n",
    "                delattr(self, attr)\n",
    "\n",
    "class ELKBackbone(nn.Module):\n",
    "    \"\"\"\n",
    "    ELK Backbone built by stacking ELKBlocks\n",
    "    `ELKBlock`ì„ ì—¬ëŸ¬ ì¸µ ìŒ“ì•„ì„œ 'ë°±ë³¸'ì„ ë§Œë“¦\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=9, d_model=128, num_layers=6, kernel_size=31, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # 'stem': ì…ë ¥ (9ì±„ë„)ì„ ëª¨ë¸ì˜ ê¸°ë³¸ ì°¨ì›(d_model=128)ìœ¼ë¡œ ë°”ê¾¸ëŠ” ì²« ë²ˆì§¸ Conv\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, d_model, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm1d(d_model),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "        layers = []\n",
    "        # ELKBlockê³¼ Dropoutì„ num_layers(6)ë²ˆ ë°˜ë³µí•´ì„œ ìŒ“ìŒ\n",
    "        for _ in range(num_layers):\n",
    "            layers.append(ELKBlock(d_model, d_model, kernel_size=kernel_size))\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        self.elk_layers = nn.Sequential(*layers)\n",
    "        self.out_channels = d_model  # ë°±ë³¸ì˜ ìµœì¢… ì¶œë ¥ ì±„ë„ ìˆ˜\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)  # (B, 9, 128) -> (B, 128, 128)\n",
    "        x = self.elk_layers(x)  # 6ê°œ ELK ë¸”ë¡ í†µê³¼ (B, 128, 128) -> (B, 128, 128)\n",
    "        return x\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Sequential Cross-Attention (ë³€ê²½ ì—†ìŒ)\n",
    "# ---------------------------\n",
    "# LayerNormì˜ ê²½ëŸ‰í™” ë²„ì „ì¸ RMSNorm\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.g = nn.Parameter(torch.ones(d))  # í•™ìŠµ ê°€ëŠ¥í•œ scale íŒŒë¼ë¯¸í„°\n",
    "    def forward(self, x):\n",
    "        # ì…ë ¥ xë¥¼ ì œê³±-í‰ê· -ì œê³±ê·¼(RMS)ìœ¼ë¡œ ì •ê·œí™”í•˜ê³  scale(g)ì„ ê³±í•¨\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.g\n",
    "\n",
    "# í‘œì¤€ ë©€í‹°í—¤ë“œ (í¬ë¡œìŠ¤) ì–´í…ì…˜ ëª¨ë“ˆ\n",
    "class MultiHeadCrossAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads=4, dropout=0.1, temperature=1.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.temperature = temperature\n",
    "\n",
    "        # Q(Query), K(Key), V(Value) ë° Outputì„ ìœ„í•œ Linear í”„ë¡œì ì…˜ ë ˆì´ì–´\n",
    "        self.q_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.k_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.v_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.out_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = (self.head_dim * temperature) ** -0.5  # ì–´í…ì…˜ ìŠ¤ì½”ì–´ ìŠ¤ì¼€ì¼ë§ ê°’\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        B, Tq, D = query.shape  # (Batch, Queryê¸¸ì´, Dim)\n",
    "        _, Tkv, _ = key.shape  # (Batch, Key/Valueê¸¸ì´, Dim)\n",
    "\n",
    "        # 1. Q, K, Vë¥¼ í”„ë¡œì ì…˜í•˜ê³  (B, T, D) -> (B, Heads, T, HeadDim)ë¡œ ë³€í˜•\n",
    "        Q = self.q_proj(query).view(B, Tq, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.k_proj(key).view(B, Tkv, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.v_proj(value).view(B, Tkv, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # 2. ì–´í…ì…˜ ìŠ¤ì½”ì–´ ê³„ì‚° (Q @ K^T)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask.unsqueeze(1) == 0, float('-inf'))\n",
    "\n",
    "        # 3. Softmax\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # 4. ê°€ì¤‘í•© (Attention @ V)\n",
    "        out = torch.matmul(attn_weights, V)\n",
    "\n",
    "        # 5. (B, Heads, Tq, HeadDim) -> (B, Tq, D)ë¡œ ì›ë³µ ë° out_proj í†µê³¼\n",
    "        out = out.transpose(1, 2).contiguous().view(B, Tq, D)\n",
    "\n",
    "        return self.out_proj(out), attn_weights\n",
    "\n",
    "# 1ì°¨ ì–´í…ì…˜: ì„¼ì„œ/ì¶•(Axis) ì „ë¬¸ê°€ í† í°ê³¼ Cross-Attention\n",
    "class ImprovedSensorCrossAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_sensors=9, num_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_sensors = num_sensors\n",
    "\n",
    "        # 'ì„¼ì„œ ì „ë¬¸ê°€ í† í°' (9ê°œ)ê³¼ 'ì¶• ì „ë¬¸ê°€ í† í°' (3ê°œ)ì„ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ë¡œ ìƒì„±\n",
    "        self.sensor_tokens = nn.Parameter(torch.randn(1, num_sensors, d_model) * 0.02)\n",
    "        self.axis_tokens = nn.Parameter(torch.randn(1, 3, d_model) * 0.02)\n",
    "\n",
    "        self.sensor_relation = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # í† í°ë“¤ë¼ë¦¬ ì •ë³´ë¥¼ ì„ê¸° ìœ„í•œ Self-Attention (SA) ëª¨ë“ˆ\n",
    "        self.sensor_sa = nn.MultiheadAttention(d_model, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.axis_sa = nn.MultiheadAttention(d_model, num_heads, dropout=dropout, batch_first=True)\n",
    "\n",
    "        # 'ë°ì´í„°(x)'ê°€ 'ì „ë¬¸ê°€ í† í°'ê³¼ ì •ë³´ë¥¼ ì„ê¸° ìœ„í•œ Cross-Attention (CA) ëª¨ë“ˆ\n",
    "        self.cross_attn = MultiHeadCrossAttention(d_model, num_heads, dropout)\n",
    "\n",
    "\n",
    "        self.axis_projection = nn.Linear(d_model, d_model)\n",
    "        self.norm_s = RMSNorm(d_model)\n",
    "        self.norm_a = RMSNorm(d_model)\n",
    "        self.norm1 = RMSNorm(d_model)\n",
    "        self.norm2 = RMSNorm(d_model)\n",
    "\n",
    "        # íŠ¸ëœìŠ¤í¬ë¨¸ì˜ FFN (Feed-Forward Network)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model), nn.GELU(), nn.Dropout(dropout),\n",
    "            nn.Linear(4 * d_model, d_model), nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):  # xì˜ ëª¨ì–‘: (B, T=128, D=128)\n",
    "        B, T, D = x.shape\n",
    "        sensor_ctx = self.sensor_tokens.expand(B, -1, -1)\n",
    "        x_pooled = x.mean(dim=1, keepdim=True)\n",
    "        sensor_ctx = sensor_ctx + self.sensor_relation(x_pooled)\n",
    "        s_norm = self.norm_s(sensor_ctx)\n",
    "        s_attn, _ = self.sensor_sa(s_norm, s_norm, s_norm)\n",
    "        sensor_ctx = sensor_ctx + s_attn\n",
    "\n",
    "        x_axis = x.mean(dim=1)\n",
    "        axis_features = [x_axis for _ in range(3)]\n",
    "        axis_stack = torch.stack(axis_features, dim=1)\n",
    "\n",
    "        axis_ctx = self.axis_tokens.expand(B, -1, -1)\n",
    "        axis_ctx = axis_ctx + self.axis_projection(axis_stack)\n",
    "        a_norm = self.norm_a(axis_ctx)\n",
    "        a_attn, _ = self.axis_sa(a_norm, a_norm, a_norm)\n",
    "        axis_ctx = axis_ctx + a_attn\n",
    "\n",
    "        combined_ctx = torch.cat([sensor_ctx, axis_ctx], dim=1)\n",
    "        residual = x\n",
    "        x_norm = self.norm1(x)\n",
    "        attn_out, _ = self.cross_attn(x_norm, combined_ctx, combined_ctx)\n",
    "        x = residual + attn_out\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "# 2ì°¨ ì–´í…ì…˜: ì‹œê°„(Temporal) ì¶•ìœ¼ë¡œ Self-Attention\n",
    "class TemporalCrossAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads=4, dropout=0.1, causal=False):\n",
    "        super().__init__()\n",
    "        # CA ëª¨ë“ˆì„ Self-Attention (SA) ìš©ë„ë¡œ ì‚¬ìš©\n",
    "        self.cross_attn = MultiHeadCrossAttention(d_model, num_heads, dropout)\n",
    "        self.norm1 = RMSNorm(d_model)\n",
    "        self.norm2 = RMSNorm(d_model)\n",
    "        self.causal = causal\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model), nn.GELU(), nn.Dropout(dropout),\n",
    "            nn.Linear(4 * d_model, d_model), nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def _create_causal_mask(self, T, device):\n",
    "        return torch.tril(torch.ones(T, T, device=device))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, D = x.shape\n",
    "        mask = self._create_causal_mask(T, x.device) if self.causal else None\n",
    "        residual = x\n",
    "        x_norm = self.norm1(x)\n",
    "\n",
    "        # Query=x, Key=x, Value=x. ì¦‰, Self-Attention\n",
    "        # \"128ê°œ íƒ€ì„ìŠ¤í…ë“¤ì•„, ë„ˆí¬ë¼ë¦¬(x, x, x) ì •ë³´ë¥¼ êµí™˜í•´\"\n",
    "        attn_out, _ = self.cross_attn(x_norm, x_norm, x_norm, mask)\n",
    "        x = residual + attn_out  # Residual\n",
    "        x = x + self.ffn(self.norm2(x))  # Residual + FFN\n",
    "        return x\n",
    "\n",
    "# 1ì°¨, 2ì°¨ ì–´í…ì…˜ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰\n",
    "class SequentialCrossAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_sensors=9, num_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.sensor_attn = ImprovedSensorCrossAttention(d_model, num_sensors, num_heads, dropout)\n",
    "        self.temporal_attn = TemporalCrossAttention(d_model, num_heads, dropout, causal=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sensor_attn(x)\n",
    "        x = self.temporal_attn(x)\n",
    "        return x\n",
    "\n",
    "# ---------------------------\n",
    "# 3) Temporal Prototype Attention (ë³€ê²½ ì—†ìŒ) ì–´í…ì…˜ìœ¼ë¡œ ì •ì œëœ íŠ¹ì§• ë§µì„ 'í”„ë¡œí† íƒ€ì…'ê³¼ ë¹„êµí•˜ì—¬ ì••ì¶•í•©ë‹ˆë‹¤.\n",
    "# ---------------------------\n",
    "# TPAì˜ ê¸°ë³¸ ë¡œì§\n",
    "class TemporalPrototypeAttention(nn.Module):\n",
    "    def __init__(self, dim, num_prototypes=16, seg_kernel=9, heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert dim % heads == 0\n",
    "        self.dim, self.heads, self.head_dim = dim, heads, dim // heads\n",
    "        self.num_prototypes = num_prototypes\n",
    "\n",
    "        # 'í”„ë¡œí† íƒ€ì…' (í•™ìŠµ ê°€ëŠ¥í•œ í–‰ë™ í…œí”Œë¦¿)ì„ íŒŒë¼ë¯¸í„°ë¡œ ìƒì„±\n",
    "        self.proto = nn.Parameter(torch.randn(num_prototypes, dim) * 0.02)\n",
    "        pad = (seg_kernel - 1) // 2\n",
    "        self.dw = nn.Conv1d(dim, dim, kernel_size=seg_kernel, padding=pad, groups=dim, bias=False)\n",
    "        self.pw = nn.Conv1d(dim, dim, kernel_size=1, bias=False)\n",
    "        # Q, K, V í”„ë¡œì ì…˜ ë ˆì´ì–´\n",
    "        self.q_proj, self.k_proj, self.v_proj, self.out_proj = [nn.Linear(dim, dim, bias=False) for _ in range(4)]\n",
    "        self.fuse = nn.Sequential(nn.Linear(dim, dim), nn.SiLU(), nn.Dropout(dropout), nn.Linear(dim, dim))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "    def _tpa_core(self, x, proto):\n",
    "        B, T, D = x.shape\n",
    "        P = proto.size(1) if proto.dim() == 3 else proto.size(0)\n",
    "        xloc = self.pw(self.dw(x.transpose(1, 2))).transpose(1, 2)\n",
    "        K, V = self.k_proj(xloc), self.v_proj(xloc)\n",
    "        Qp = self.q_proj(proto) if proto.dim() == 3 else self.q_proj(proto).unsqueeze(0).expand(B, -1, -1)\n",
    "\n",
    "        def split_heads(t, is_kv=False):\n",
    "            shape = (B, T if is_kv else P, self.heads, D // self.heads)\n",
    "            return t.view(*shape).transpose(1, 2)\n",
    "\n",
    "        Qh, Kh, Vh = split_heads(Qp, False), split_heads(K, True), split_heads(V, True)\n",
    "        scores = torch.matmul(Qh, Kh.transpose(-2, -1)) * self.scale\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        proto_tokens = torch.matmul(attn, Vh).transpose(1, 2).contiguous().view(B, P, D)\n",
    "        z = self.fuse(proto_tokens.mean(dim=1) + proto_tokens.max(dim=1).values)\n",
    "        z = self.out_proj(z)\n",
    "        aux = {\"attn\": attn, \"align_peak_mean\": attn.amax(dim=-1).mean().detach()}\n",
    "        return z, aux  # ìµœì¢… ì••ì¶• ë²¡í„° z ë°˜í™˜\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._tpa_core(x, self.proto)\n",
    "\n",
    "\n",
    "# TPAì˜ 'í´ë˜ìŠ¤ ì¡°ê±´ë¶€' ì—…ê·¸ë ˆì´ë“œ ë²„ì „\n",
    "class ClassConditionalTPA(TemporalPrototypeAttention):\n",
    "    def __init__(self, dim, num_classes, p_shared=8, p_class=4, **kw):\n",
    "        # 'ê³µìœ ' í”„ë¡œí† íƒ€ì… 8ê°œ + 'í´ë˜ìŠ¤ë³„' í”„ë¡œí† íƒ€ì… (4 * 6=24ê°œ) = ì´ 32ê°œ\n",
    "        super().__init__(dim, num_prototypes=p_shared + num_classes * p_class, **kw)\n",
    "        self.p_shared, self.p_class, self.num_classes = p_shared, p_class, num_classes\n",
    "\n",
    "    # íŠ¹ì • í´ë˜ìŠ¤(cls_idx)ì˜ ì „ìš© í”„ë¡œí† íƒ€ì… 4ê°œë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜\n",
    "    def _slice_class(self, cls_idx):\n",
    "        base = self.p_shared + cls_idx * self.p_class\n",
    "        return self.proto[base: base + self.p_class]\n",
    "\n",
    "    def forward(self, x, y=None, logits=None):\n",
    "        B = x.size(0)\n",
    "        shared = self.proto[:self.p_shared]\n",
    "        if self.training and y is not None:\n",
    "            # ğŸ§  í›ˆë ¨ ì‹œ: ì •ë‹µ(y)ì„ ì•„ë‹ˆê¹Œ, ì •ë‹µ í´ë˜ìŠ¤ì˜ ì „ìš© í”„ë¡œí† íƒ€ì…ì„ ê°€ì ¸ì˜´\n",
    "            cls_proto = torch.stack([self._slice_class(y[i].item()) for i in range(B)], 0)\n",
    "            # (ê³µí†µ 8ê°œ + ì •ë‹µ ì „ìš© 4ê°œ) = 12ê°œ í”„ë¡œí† íƒ€ì…ì„ Queryë¡œ ì‚¬ìš©\n",
    "            proto = torch.cat([shared.unsqueeze(0).expand(B, -1, -1), cls_proto], dim=1)\n",
    "        else:\n",
    "            # ğŸš€ í‰ê°€ ì‹œ: ì •ë‹µ(y)ì„ ëª¨ë¦„\n",
    "            if logits is None:\n",
    "                # (ì˜ˆì™¸) logitsë„ ì—†ìœ¼ë©´ ê³µí†µ í”„ë¡œí† íƒ€ì… 8ê°œë§Œ ì‚¬ìš©\n",
    "                proto = shared.unsqueeze(0).expand(B, -1, -1)\n",
    "            else:\n",
    "                # ë°±ë³¸ì´ ì˜ˆì¸¡í•œ logits (B, 6)ë¥¼ softmax(pi) (B, 6)ë¡œ ë³€í™˜\n",
    "                pi = logits.softmax(dim=-1)\n",
    "                # 6ê°œ í´ë˜ìŠ¤ì˜ ì „ìš© í”„ë¡œí† íƒ€ì… ë±…í¬ (6, 4, D)\n",
    "                class_bank = torch.stack([self._slice_class(c) for c in range(self.num_classes)], 0)\n",
    "                # (B, 6) @ (6, 4, D) -> (B, 4, D)\n",
    "                # 6ê°œ í´ë˜ìŠ¤ì˜ ì „ìš© í”„ë¡œí† íƒ€ì… 4ê°œë¥¼ logits í™•ë¥ ë¡œ 'ì„ì–´ì„œ' ë§Œë“¦\n",
    "                mixed = torch.einsum('bc,cpd->bpd', pi, class_bank)\n",
    "                # (ê³µí†µ 8ê°œ + ì„ì¸ 4ê°œ) = 12ê°œ í”„ë¡œí† íƒ€ì…ì„ Queryë¡œ ì‚¬ìš©\n",
    "                proto = torch.cat([shared.unsqueeze(0).expand(B, -1, -1), mixed], dim=1)\n",
    "\n",
    "        return self._tpa_core(x, proto)  # ì„ íƒëœ 'proto'ë¥¼ Queryë¡œ ì‚¬ìš©í•˜ì—¬ TPA í•µì‹¬ ë¡œì§ ìˆ˜í–‰\n",
    "\n",
    "# ---------------------------\n",
    "# 4) Loss Function (ë³€ê²½ ì—†ìŒ)\n",
    "# ---------------------------\n",
    "class ImprovedClsLoss(nn.Module):\n",
    "    def __init__(self, use_focal=True, alpha=0.25, gamma=2.0, init_loss_weight=0.4, label_smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.use_focal, self.alpha, self.gamma = use_focal, alpha, gamma\n",
    "        self.init_loss_weight, self.label_smoothing = init_loss_weight, label_smoothing\n",
    "\n",
    "    def forward(self, logits, labels, aux_info, aux_weight_multiplier=1.0):\n",
    "        n_classes = logits.size(-1)\n",
    "        if self.label_smoothing > 0:\n",
    "            one_hot = F.one_hot(labels, num_classes=n_classes).float()\n",
    "            smooth_label = one_hot * (1 - self.label_smoothing) + self.label_smoothing / n_classes\n",
    "            if self.use_focal:\n",
    "                log_probs = F.log_softmax(logits, dim=-1)\n",
    "                pt = torch.exp(log_probs)\n",
    "                focal_weight = (1 - pt) ** self.gamma\n",
    "                loss = -(self.alpha * focal_weight * smooth_label * log_probs).sum(dim=-1).mean()\n",
    "            else:\n",
    "                loss = -(smooth_label * F.log_softmax(logits, dim=-1)).sum(dim=-1).mean()\n",
    "        else:\n",
    "            ce = F.cross_entropy(logits, labels, reduction=\"none\")\n",
    "            if self.use_focal:\n",
    "                pt = torch.exp(-ce)\n",
    "                loss = (self.alpha * (1 - pt)**self.gamma * ce).mean()\n",
    "            else:\n",
    "                loss = ce.mean()\n",
    "\n",
    "        total_loss = loss\n",
    "        if \"logits_init\" in aux_info:\n",
    "            loss_init = F.cross_entropy(aux_info[\"logits_init\"], labels)\n",
    "            total_loss = loss + self.init_loss_weight * loss_init\n",
    "\n",
    "        return total_loss, {\n",
    "            \"classification_loss\": float(loss.item()),\n",
    "            \"total_loss\": float(total_loss.item()),\n",
    "            \"align_peak_mean\": float(aux_info.get(\"align_peak_mean\", 0.)),\n",
    "        }\n",
    "\n",
    "# ---------------------------\n",
    "# 5) Hybrid Model (TCN -> ELK) ëª¨ë“  ë¶€í’ˆì„ ì¡°ë¦½í•˜ëŠ” ìµœì¢… ëª¨ë¸\n",
    "# ---------------------------\n",
    "class ELK_SequentialAttn_TPA(nn.Module):\n",
    "    def __init__(self, nvars, seq_len, num_classes,\n",
    "                 num_elk_layers, elk_kernel_size, # ELK íŒŒë¼ë¯¸í„°\n",
    "                 d_model, heads, dropout,\n",
    "                 num_prototypes, seg_kernel, p_shared, p_class,\n",
    "                 use_class_conditional, use_cross_attention):\n",
    "        super().__init__()\n",
    "        self.use_class_conditional = use_class_conditional\n",
    "        self.use_cross_attention = use_cross_attention\n",
    "        \n",
    "        # ë¶€í’ˆ 1: ELKBackbone (ì„¹ì…˜ 1)\n",
    "        # TCN Backbone -> ELK Backbone ìœ¼ë¡œ êµì²´\n",
    "        self.backbone = ELKBackbone(\n",
    "            in_channels=nvars,\n",
    "            d_model=d_model,\n",
    "            num_layers=num_elk_layers,\n",
    "            kernel_size=elk_kernel_size,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        # ë°±ë³¸ ì¶œë ¥ ì±„ë„(128) -> ì–´í…ì…˜ ì…ë ¥ ì±„ë„(128)ë¡œ ë§ì¶°ì£¼ëŠ” 1x1 Conv\n",
    "        self.proj = nn.Conv1d(self.backbone.out_channels, d_model, kernel_size=1)\n",
    "\n",
    "        # ë¶€í’ˆ 2: SequentialCrossAttention (ì„¹ì…˜ 2)\n",
    "        if use_cross_attention:\n",
    "            self.cross_attn = SequentialCrossAttention(\n",
    "                d_model, nvars, num_heads=heads, dropout=dropout\n",
    "            )\n",
    "\n",
    "        # ë¶€í’ˆ 3: TemporalPrototypeAttention (ì„¹ì…˜ 3)\n",
    "        if use_class_conditional:\n",
    "            self.tpa = ClassConditionalTPA(\n",
    "                d_model, num_classes, p_shared, p_class,\n",
    "                seg_kernel=seg_kernel, heads=heads, dropout=dropout  # ì¡°ê±´ë¶€ TPA\n",
    "            )\n",
    "        else:\n",
    "            self.tpa = TemporalPrototypeAttention(\n",
    "                d_model, num_prototypes, seg_kernel, heads, dropout  # ì¼ë°˜ TPA\n",
    "            )\n",
    "\n",
    "        self.head_init = nn.Linear(d_model, num_classes)  # ë¶„ë¥˜ í—¤ë“œ 1: ë°±ë³¸ ì¶œë ¥ì„ ìœ„í•œ ì´ˆê¸° ë¶„ë¥˜ê¸°\n",
    "        self.head_final = nn.Linear(d_model, num_classes)  # ë¶„ë¥˜ í—¤ë“œ 2: TPA ì¶œë ¥ì„ ìœ„í•œ ìµœì¢… ë¶„ë¥˜ê¸°\n",
    "\n",
    "    def forward(self, x, labels=None):\n",
    "        x = self.backbone(x) # self.tcn -> self.backbone\n",
    "        x = self.proj(x)\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        x_pooled = x.mean(dim=1)\n",
    "        logits_init = self.head_init(x_pooled)\n",
    "\n",
    "        if self.use_cross_attention:\n",
    "            x = self.cross_attn(x)\n",
    "\n",
    "        if self.use_class_conditional:\n",
    "            z, aux = self.tpa(x, y=labels, logits=logits_init)\n",
    "        else:\n",
    "            z, aux = self.tpa(x)\n",
    "\n",
    "        logits_final = self.head_final(z)\n",
    "        aux['logits_init'] = logits_init\n",
    "        return logits_final, aux\n",
    "\n",
    "    def reparameterize(self):\n",
    "        \"\"\"ì¶”ë¡ ì„ ìœ„í•´ ëª¨ë“  ELK ë¸”ë¡ì„ ì¬ë§¤ê°œë³€ìˆ˜í™”í•©ë‹ˆë‹¤.\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, ELKBlock):\n",
    "                m.reparameterize()\n",
    "\n",
    "# ---------------------------\n",
    "# 6) Train / Eval (ë³€ê²½ ì—†ìŒ)\n",
    "# ---------------------------\n",
    "def train_epoch(model, loader, criterion, optim, scheduler, device, accumulation_steps=4):\n",
    "    model.train()\n",
    "    tot, correct, total = 0.0, 0, 0\n",
    "    optim.zero_grad()\n",
    "    for i, (x, y) in enumerate(loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits, aux = model(x, labels=y)\n",
    "        loss, _ = criterion(logits, y, aux, 1.0)\n",
    "        loss = loss / accumulation_steps\n",
    "        loss.backward()\n",
    "\n",
    "        if (i + 1) % accumulation_steps == 0 or i == len(loader) - 1:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optim.step()\n",
    "            if scheduler: scheduler.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "        tot += loss.item() * accumulation_steps\n",
    "        pred = logits.argmax(1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    return tot / len(loader), 100 * correct / total\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    tot, correct, total = 0.0, 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits, aux = model(x, labels=None)\n",
    "        loss, _ = criterion(logits, y, aux, 1.0)\n",
    "        tot += loss.item()\n",
    "        pred = logits.argmax(1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    return tot / len(loader), 100 * correct / total\n",
    "\n",
    "# ---------------------------\n",
    "# 7) Main (ëª¨ë¸ ë° íŒŒë¼ë¯¸í„° ìˆ˜ì •)\n",
    "# ---------------------------\n",
    "def main():\n",
    "    import gc\n",
    "    torch.cuda.empty_cache(); gc.collect()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(\"UCI-HAR Training: ELK Backbone + Sequential Cross-Attention + TPA\")\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "    uci_root = \"/content/drive/MyDrive/Colab Notebooks/UCI-HAR/UCI-HAR\"\n",
    "\n",
    "    if not os.path.exists(uci_root):\n",
    "        print(f\"Error: UCI-HAR data not found at '{uci_root}'\")\n",
    "        return\n",
    "\n",
    "    train_ds = UCIHARDataset(uci_root, \"train\")\n",
    "    test_ds = UCIHARDataset(uci_root, \"test\")\n",
    "\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        np.arange(len(train_ds)), test_size=0.2, random_state=42, stratify=train_ds.y\n",
    "    )\n",
    "\n",
    "    mu, sd = fit_channel_stats(train_ds.X[train_indices])\n",
    "    train_ds.set_stats((mu, sd))\n",
    "    test_ds.set_stats((mu, sd)) # í…ŒìŠ¤íŠ¸ì…‹ì—ë„ í›ˆë ¨ì…‹ í†µê³„ ì ìš©\n",
    "\n",
    "    # Validationì…‹ì„ ìœ„í•œ ë³„ë„ Dataset ê°ì²´ ìƒì„± í›„ í†µê³„ ì ìš©\n",
    "    val_ds = UCIHARDataset(uci_root, \"train\")\n",
    "    val_ds.set_stats((mu, sd))\n",
    "\n",
    "    train_subset = Subset(train_ds, train_indices)\n",
    "    val_subset = Subset(val_ds, val_indices)\n",
    "\n",
    "    print(f\"\\nDataset Split: Train={len(train_subset)}, Val={len(val_subset)}, Test={len(test_ds)}\")\n",
    "\n",
    "    # ëª¨ë¸ ì •ì˜ (ELK íŒŒë¼ë¯¸í„° ì‚¬ìš©)\n",
    "    model = ELK_SequentialAttn_TPA(\n",
    "        nvars=9,\n",
    "        seq_len=128,\n",
    "        num_classes=6,\n",
    "        d_model=128,\n",
    "        heads=4,\n",
    "        dropout=0.2,\n",
    "        # --- ELK Parameters ---\n",
    "        num_elk_layers=6,\n",
    "        elk_kernel_size=31,\n",
    "        # --- TPA Parameters ---\n",
    "        num_prototypes=8,\n",
    "        seg_kernel=3,\n",
    "        p_shared=6,\n",
    "        p_class=4,\n",
    "        use_class_conditional=True,\n",
    "        use_cross_attention=True\n",
    "    ).to(device)\n",
    "\n",
    "    print(f\"\\nModel parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(\"Architecture: ELK Backbone + Sequential Cross-Attention + TPA\")\n",
    "\n",
    "    criterion = ImprovedClsLoss(use_focal=True, alpha=0.25, gamma=2.0, init_loss_weight=0.4, label_smoothing=0.1)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.05)\n",
    "\n",
    "    batch_size = 64\n",
    "    accumulation_steps = 2\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=batch_size*2, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size*2, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "    max_epochs = 60\n",
    "    total_steps = (len(train_loader) // accumulation_steps) * max_epochs\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, max_lr=2e-3, total_steps=total_steps, pct_start=0.2\n",
    "    )\n",
    "\n",
    "    best_val_acc, best_epoch = 0.0, 0\n",
    "    print(f\"\\nStarting training for {max_epochs} epochs...\")\n",
    "    print(f\"Effective batch size: {batch_size * accumulation_steps}\")\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"\\nEpoch {epoch}/{max_epochs} | LR: {lr:.6f}\")\n",
    "        tr_loss, tr_acc = train_epoch(model, train_loader, criterion, optimizer, scheduler, device, accumulation_steps)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "        print(f\"  Train: loss {tr_loss:.4f} | acc {tr_acc:.2f}%\")\n",
    "        print(f\"  Val  : loss {val_loss:.4f} | acc {val_acc:.2f}%\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc, best_epoch = val_acc, epoch\n",
    "            torch.save(model.state_dict(), \"best_model_elk_valsel.pth\")\n",
    "            print(f\"  âœ“ New best validation accuracy: {best_val_acc:.2f}% (model saved)\")\n",
    "\n",
    "    print(f\"\\nTraining completed! Best val acc: {best_val_acc:.2f}% at epoch {best_epoch}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL TEST SET EVALUATION (Unseen Data)\")\n",
    "    print(\"â†’ Reparameterizing ELK blocks for inference speed-up...\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # ì €ì¥ëœ ìµœê³ ì˜ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¨ í›„, ì¬ë§¤ê°œë³€ìˆ˜í™” ìˆ˜í–‰\n",
    "    model.load_state_dict(torch.load(\"best_model_elk_valsel.pth\"))\n",
    "    model.reparameterize()\n",
    "\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "    print(f\"\\nFinal Results on Test Set:\")\n",
    "    print(f\"  - Test Loss    : {test_loss:.4f}\")\n",
    "    print(f\"  - Test Accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
